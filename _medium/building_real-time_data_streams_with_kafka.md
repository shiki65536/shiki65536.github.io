---
layout: default
title: "Building Real-time Data Streams with Kafka"
date: Sun, 21 Sep 2025 04:13:20 +0000
excerpt: "Project BackgroundThis project"
link: "https://medium.com/@shiki65536/building-real-time-data-streams-with-kafka-360c91a5d047?source=rss-374d8f1302a3------2"
image: "https://cdn-images-1.medium.com/max/1024/1*KfemSGYxVkSHaZwJ-T-a1g.png"
tags: ["docker", "kafka", "data-streaming"]
---
Project BackgroundThis project originated from my Big Data course back in Monash. The original task was analyzing data from a fictional e-commerce platform, containing customer information, browsing behavior, and transaction records — approximately 5 million records total.The Real-time ChallengeE-commerce platforms generate continuous streams of user behavior data. Every click, page view, and cart addition represents a real-time business opportunity. My challenge: transform static CSV clickstream data into realistic real-time streams using Kafka to simulate realistic streaming behavior while maintaining memory efficiency and proper temporal distribution.Key requirements:Memory-efficient CSV reading (no loading entire file)Realistic batch timing with proper timestampsContinuous data flow simulationSolution ArchitectureGenerator patternfor memory efficiency : reads CSV rows on-demand rather than loading everythingTemporal distribution: spreads batch events evenly across time intervals to avoid traffic spikesTimestamp injection: adds `ts` field for event-time processing in downstream Spark StreamingRealistic batching: random batch sizes (500–1000 records) simulate natural traffic variationsTechnical Implementation**The producer uses a generator-based approach to handle large datasets efficiently. Each batch gets timestamped and distributed across a 5-second interval to simulate realistic user behavior patterns.The timestamp injection is critical — it converts historical data into event-time aware records, essential for Spark Streaming’s watermark mechanisms.def stream_from_csv(producer, topic, csv_path, batch_min=500, batch_max=1000, pause_s=5): def row_generator(path): while True: # Infinite loop with open(path, &#39;r&#39;, newline=&#39;&#39;, encoding=&#39;utf-8&#39;) as fh: reader = csv.DictReader(fh) for row in reader: yield row  gen = row_generator(csv_path) while True: batch_size = random.randint(batch_min, batch_max) ts_start = int(time.time()) per_second = max(1, batch_size // pause_s)  for i in range(batch_size): row = next(gen) ts = ts_start + (i // per_second) # Spread timestamps row[&#39;ts&#39;] = ts producer.send(topic, row)  producer.flush() time.sleep(pause_s)Consumer Verificationdef consume_to_dataframe(broker, topic, timeout_ms=10000, max_messages=100): consumer = KafkaConsumer( topic, bootstrap_servers=[broker], auto_offset_reset=&#39;earliest&#39;, value_deserializer=lambda m: json.loads(m.decode(&#39;utf-8&#39;)) )  rows = [] for i, msg in enumerate(consumer): rows.append(msg.value) if max_messages and i+1 &gt;= max_messages: break  consumer.close() return pd.DataFrame(rows)Bridges Kafka streaming with pandas DataFrames for quick verification.Common issuesProducer failing silently (serialization errors)Consumer starting from latest offset while producer idleDocker networking misconfiguration
