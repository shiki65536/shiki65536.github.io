I"R<p>Project BackgroundThis project originated from my Big Data course back in Monash. The original task was analyzing data from a fictional e-commerce platform, containing customer information, browsing behavior, and transaction records — approximately 5 million records total.The Real-time ChallengeE-commerce platforms generate continuous streams of user behavior data. Every click, page view, and cart addition represents a real-time business opportunity. My challenge: transform static CSV clickstream data into realistic real-time streams using Kafka to simulate realistic streaming behavior while maintaining memory efficiency and proper temporal distribution.Key requirements:Memory-efficient CSV reading (no loading entire file)Realistic batch timing with proper timestampsContinuous data flow simulationSolution ArchitectureGenerator patternfor memory efficiency : reads CSV rows on-demand rather than loading everythingTemporal distribution: spreads batch events evenly across time intervals to avoid traffic spikesTimestamp injection: adds <code class="language-plaintext highlighter-rouge">ts</code> field for event-time processing in downstream Spark StreamingRealistic batching: random batch sizes (500–1000 records) simulate natural traffic variationsTechnical Implementation**The producer uses a generator-based approach to handle large datasets efficiently. Each batch gets timestamped and distributed across a 5-second interval to simulate realistic user behavior patterns.The timestamp injection is critical — it converts historical data into event-time aware records, essential for Spark Streaming’s watermark mechanisms.def stream_from_csv(producer, topic, csv_path, batch_min=500, batch_max=1000, pause_s=5): def row_generator(path): while True: # Infinite loop with open(path, 'r', newline='', encoding='utf-8') as fh: reader = csv.DictReader(fh) for row in reader: yield row  gen = row_generator(csv_path) while True: batch_size = random.randint(batch_min, batch_max) ts_start = int(time.time()) per_second = max(1, batch_size // pause_s)  for i in range(batch_size): row = next(gen) ts = ts_start + (i // per_second) # Spread timestamps row['ts'] = ts producer.send(topic, row)  producer.flush() time.sleep(pause_s)Consumer Verificationdef consume_to_dataframe(broker, topic, timeout_ms=10000, max_messages=100): consumer = KafkaConsumer( topic, bootstrap_servers=[broker], auto_offset_reset='earliest', value_deserializer=lambda m: json.loads(m.decode('utf-8')) )  rows = [] for i, msg in enumerate(consumer): rows.append(msg.value) if max_messages and i+1 &gt;= max_messages: break  consumer.close() return pd.DataFrame(rows)Bridges Kafka streaming with pandas DataFrames for quick verification.Common issuesProducer failing silently (serialization errors)Consumer starting from latest offset while producer idleDocker networking misconfiguration</p>
:ET